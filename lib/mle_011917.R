
# get passed arguments
sample          <- Sys.getenv("sample")
maxCnvSize      <- as.numeric(Sys.getenv("maxCnvSize"))
pdfDir          <- Sys.getenv("pdfDir")
libraries       <- strsplit(Sys.getenv("libraries"), ",")[[1]]
libraryNs       <- strsplit(Sys.getenv("libraryNs"), ",")[[1]]
tLenFile        <- Sys.getenv("tLenFile")
min_ks_pairs    <- as.numeric(Sys.getenv("min_ks_pairs"))
nlog_ks_alpha   <- -log10(as.numeric(Sys.getenv("ks_alpha")))

# calculate derived values
minTLen <- -maxCnvSize;
maxTLen <- 2 * maxCnvSize;

# get the TLEN data for all interrogated bin positions
write("  loading bin TLEN data", file=stderr())
bins <- read.table(tLenFile, header=FALSE, sep="\t", stringsAsFactors=FALSE)
colnames(bins) <- c('chrom', 'binI', 'nFrags', 'tLens', 'libNs')

# get the expected TLEN frequency distributions
#   generated by svtools pdf for the same libraries/sample
#   '0' is for all libraries together, others start at '1'
write("  loading library TLEN distributions", file=stderr())
libAll    <- '0'
nLib      <- length(libraries)
dist      <- list()
dist_freq <- list()
load_dist <- function(lib, libN){
    distFile      <- paste(pdfDir, "/svtools.pdf.", sample, ".", lib, ".gz", sep="")
    tmp           <- read.table(distFile, header=FALSE, sep="\t", stringsAsFactors=FALSE)
    colnames(tmp) <- c('tLen', 'freq')
    dist[[libN]]      <<- tmp[tmp$tLen>=minTLen&tmp$tLen<=maxTLen,]
    dist_freq[[libN]] <<- stepfun(dist[[libN]]$tLen, c(0, dist[[libN]]$freq))    
}
for (i in 1:nLib){ load_dist(libraries[i], libraryNs[i]) }
if(nLib>1){ load_dist('all', libAll) } else { dist[[libAll]] <- dist[['1']] }

# set the bounds on allowed CNV sizes
write("  setting CNV size bounds", file=stderr())
sizeBin    <- dist[[libAll]][2,'tLen'] - dist[[libAll]][1,'tLen']
cnvSizes   <- list(
    'del'  =  seq(0, maxCnvSize, sizeBin), 
    'dup'  =  seq(-maxCnvSize, 0, sizeBin),
    'ins'  =  seq(-maxCnvSize, 0, sizeBin),
    'del_dup' = seq(-maxCnvSize, maxCnvSize, sizeBin),
    'del_ins' = seq(-maxCnvSize, maxCnvSize, sizeBin)
)
nCnvSizes  <- list(
    'del'  =  length(cnvSizes[['del']]),
    'dup'  =  length(cnvSizes[['dup']]),
    'ins'  =  length(cnvSizes[['ins']]),
    'del_dup' = length(cnvSizes[['del_dup']]),
    'del_ins' = length(cnvSizes[['del_ins']])
)
fragSizes <- cnvSizes[['del']]
tLens     <- seq(minTLen, maxTLen, sizeBin)
nTLens    <- length(tLens)

# establish weighted frequency for non-CNV regions
#   dist has size frequency by fragment _count_
#   need to weight by abs(TLEN) to get the probablity that 
#   a given position will cross a mapped span of a given length
write("  calculating TLEN-weighted reference distribution", file=stderr())
weightFreqs <- function(freq, tLen){
    wF <- freq * abs(tLen)
    wF / sum(wF)
}
dist[[libAll]]$freq <- weightFreqs(dist[[libAll]]$freq, dist[[libAll]]$tLen)
    
# set the cdf used in the first-pass test of a set of TLENs against H0
all_cdf <- stepfun(dist[[libAll]]$tLen, c(0, cumsum(dist[[libAll]]$freq)))

# establish weighted frequency for CNV regions of various sizes, per library
write("  calculating TLEN-weighted alternative distributions", file=stderr())
dist_pdf <- list()
for (libN in libraryNs){
    dist_pdf[[libN]] <- list()
    for (cnvSize in cnvSizes[['del_dup']]){
        wF <- weightFreqs(dist_freq[[libN]](tLens-cnvSize), tLens)
        dist_pdf[[libN]][[as.character(cnvSize)]] <- stepfun(tLens, c(0, wF))
    }
} 

# define a combined reference CDF for a given set of library pairs
#   ks.test uses this to test the set of pairs against CNV size == 0
lb       <- character()
nFrags   <- 0
libFrac  <- data.frame()
ref_pdf  <- function(){}
ref_cum  <- numeric()
ref_cdf  <- function(){}
get_sized_pdf <- function(cnvSize){
    stepfun(tLens, c(0, sapply(tLens, function(tLen){
        sum(sapply(1:nrow(libFrac), function(i){
            dist_pdf[[libFrac[i,1]]][[as.character(cnvSize)]](tLen) * libFrac[i,2]
        }))   
    })))
}
set_ref_cdf <- function(){
    libFrac <<- aggregate(lb, list(lb), function(x){ length(x) / nFrags } )
    ref_pdf <<- get_sized_pdf(0)
    ref_cum <<- cumsum(ref_pdf(tLens))
    ref_cdf <<- stepfun(tLens, c(0, ref_cum))  
}
# and again for CNV size determined from MLE
alt_cdf_2 <- function(){}
set_alt_cdf_2 <- function(cnvSize, cnvFrac){
    alt_pdf <- get_sized_pdf(cnvSize)
    alt_cum <- cumsum(sapply(tLens, function(tLen){
        ref_pdf(tLen) * (1 - cnvFrac) + alt_pdf(tLen) * cnvFrac
    }))
    alt_cdf_2 <<- stepfun(tLens, c(0, alt_cum)) 
}


# set median reference point used to determine most likely alt allele
median_ref_tLen <- 0
set_median_ref_tLen <- function(){
    i_at_50 <- max(which(ref_cum <= 0.5))
    median_ref_tLen <<- if(ref_cum[i_at_50] == 0.5){
        tLens[i_at_50]
    } else {
        mean(tLens[i_at_50], tLens[i_at_50 + 1])
    }      
}

# handle calculations of probabilities in mixed models,
#   i.e. ref + alt, with alt weighted by cnvFrac
# 2-parameter MLE, i.e. one ref, one alt (includes wt/alt, alt/alt, alt/-)
tL <- numeric()
dist_pdf_tL_ref <- numeric()
dist_pdf_tL_alt <- list()
fill_nll_2 <- Vectorize(function(cnvSizeI, cnvFrac){
    -sum(log(
        dist_pdf_tL_ref * (1 - cnvFrac) + dist_pdf_tL_alt[[cnvSizeI]] * cnvFrac
    ))
})
#alt_cdf_2 <- function(tL, cnvFrac){
#    #ref_cdf(tL) * (1 - cnvFrac) + ref_cdf(tL - cnvSize) * cnvFrac
#    ref_cdf(tL) * (1 - cnvFrac) + alt_cdf(tL) * cnvFrac
#}
# 3-parameter MLE, i.e. two alt (includes alt1/alt2)
max_frags_3 <- 0 # temporarily turned off this MLE by setting to 0
fill_nll_3 <- Vectorize(function(cnvSizeI_1, cnvSizeI_2, cnvFrac_2){
    -sum(log(
        dist_pdf_tL_alt[[cnvSizeI_1]] * (1 - cnvFrac_2) + dist_pdf_tL_alt[[cnvSizeI_2]] * cnvFrac_2
    ))
})
alt_cdf_3 <- function(tL, cnvSize_1, cnvSize_2, cnvFrac_2){
    ref_cdf(tL - cnvSize_1) * (1 - cnvFrac_2) + ref_cdf(tL - cnvSize_2) * cnvFrac_2  
}

# perform the MLE on each bin, i.e. each discrete genome position
parse_bin <- function(bin){

    # check to see if position has sufficient data to proceed
    nFrags <<- as.numeric(bin[2])
    res <- if(nFrags < min_ks_pairs){
        c(0, 0, 0, 0, 0, 'ref')
    } else {

        # use KS test to determine if bin is consistent with genome TLEN distribution
        # do first test against merged library CDF, fastest for most bins        
        tL <<- as.numeric(strsplit(bin[3], ",")[[1]])        
        nlog_ks_p_ref <- round(-log10(suppressWarnings(ks.test(tL, all_cdf)$p.value)), 3)
        if(nlog_ks_p_ref < nlog_ks_alpha){
            c(nlog_ks_p_ref, nlog_ks_p_ref, 0, 0, 0, 'ref')
        } else {         

            # only if merged library CDF fails proceed on to lib-specific CDF
            lb <<- strsplit(bin[4], ",")[[1]]
            set_ref_cdf()
            nlog_ks_p_ref <- round(-log10(suppressWarnings(ks.test(tL, ref_cdf)$p.value)), 3)
            if(nlog_ks_p_ref < nlog_ks_alpha){
                c(nlog_ks_p_ref, nlog_ks_p_ref, 0, 0, 0, 'ref') 
            } else {
                
                # does it have substantial negative fragments?
                # yes - size a duplication from negative fragments
                #       size positive fragments as single size
                # no - size positive fragments as single size with frac
                
                
                
                # determine the most likely type of a single alt allele
                set_median_ref_tLen()            
                cnvType <- if(length(tL[tL <= median_ref_tLen]) / nFrags < 0.5){ # presumptive loss/del vs. reference
                    'del'
                } else { # presumptive gain vs. reference
                    if(length(tL[tL<0]) >= 1){ # has neg TLENs, presumptive dup
                        'dup'
                    } else {
                        'ins'
                    }
                }
                
                # establish the null model, wherein fragSize = TLEN, i.e. all frags are reference
                #   allow negative TLEN, i.e. fragSize, in this model to account for spurious mis-mappings
                dist_pdf_tL_ref <<- sapply(1:nFrags, function(i){
                    dist_pdf[[lb[i]]][['0']](tL[i])
                })
                
                # establish the alt models, wherein fragSize = TLEN - cnvSize
                # where cnvSize is >0 for deletion and <0 for insertion and duplication
                # with restrictions that:
                #   fragSize can never be negative
                #   fragSize must be greater than cnvSize for insertion
                
                
                cnvType <- 'del_dup'
                
                
                dist_pdf_tL_alt <<- lapply(cnvSizes[[cnvType]], function(cnvSize){
                    fragSizes   <- tL - cnvSize
                    minFragSize <- if(cnvType == 'ins'){ cnvSize } else { 0 }            
                    sapply(1:nFrags, function(i){
                        
                        if(fragSizes[i] < minFragSize){ 1e-5 } else {
                            
                            dist_pdf[[lb[i]]][[as.character(cnvSize)]](tL[i])
                        }
                    })
                })

                # perform 2D maximum likelihood estimation of alt cnvSize and nCnvFrags
                nll_2     <- outer(1:nCnvSizes[[cnvType]], 1:nFrags/nFrags, fill_nll_2)
                min_cell  <- which(nll_2 == min(nll_2), arr.ind=TRUE)[1,]
                cnvSize   <- cnvSizes[[cnvType]][min_cell[1]]
                nCnvFrags <- min_cell[2]
                cnvFrac   <- nCnvFrags/nFrags
                set_alt_cdf_2(cnvSize, cnvFrac)
                nlog_ks_p_alt <- round(-log10(suppressWarnings(ks.test(tL, alt_cdf_2)$p.value)), 3)
                
                ####################
                
                # 96 shows a mess, likely due to false underweighting of zero?
                
                if(nFrags == 96){
                    write(cnvSizes[[cnvType]][min_cell[1]], file=stderr())
                    write(min_cell[2]/nFrags, file=stderr())
                    
                    write(paste(tL,sep=" "), file=stderr())
                    
                    df <- data.frame(tL=tLens)
                    act_fun <- ecdf(tL)
                    df$act  <- act_fun(tLens)
                    
                    #set_alt_cdf_2(-11800, 0.25)
                    
                    df$h0   <- ref_cdf(tLens)
                    df$ha   <- alt_cdf_2(tLens)
                    write.table(
                        df,
                        file      = stderr(),
                        quote     = FALSE,
                        sep       = "\t",
                        row.names = FALSE,
                        col.names = FALSE
                    )      
                }

                # return the results = nlog_ks_p_ref, nlog_ks_p_alt, cnvSize_1, cnvSize_2, nCnvFrags_2, cnvType
                if(nlog_ks_p_alt < nlog_ks_alpha | nFrags > max_frags_3){
                    c(min(nlog_ks_p_ref, 15), min(nlog_ks_p_alt, 15), 0, cnvSize, nCnvFrags, cnvType)
                    
                # as needed (rarely) perform 3D maximum likelihood estimation of two alt alleles    
                } else {
                    
                    # TODO: increase speed by further downsampling frags to harser limit, e.g. max 50 frags
                    # for now, just add a category of "cmp"
                    
                    # reset cnvType to allow more choices for two alleles, but still ignore dup if no negative TLENs
                    cnvType   <- if(cnvType == 'dup'){ 'del_dup' } else { 'del_ins' }
                    cnvSizes  <- cnvSizes[[cnvType]]
                    nCnvSizes <- nCnvSizes[[cnvType]]
                    dist_pdf_tL_alt <<- lapply(cnvSizes, function(cnvSize){
                        fragSizes   <- tL - cnvSize                    
                        minFragSize <- if(cnvType == 'del_ins'){ cnvSize } else { 0 } 
                        sapply(1:nFrags, function(i){
                            if(fragSizes[i] < minFragSize){ 0 } else {
                                dist_pdf[[lb[i]]][[as.character(fragSizes[i])]](tL[i])
                            }  
                        })   
                    }) 
                    
                    # run the 3D MLE
                    min_3 <- c(1e9)                
                    for (nCnvFrags_2 in 1:nFrags){
                        nll_3 <- outer(1:nCnvSizes, 1:nCnvSizes, fill_nll_3, nCnvFrags_2/nFrags)                
                        min_cell <- which(nll_3 == min(nll_3), arr.ind=TRUE)[1,]                
                        min_nll <- nll_3[min_cell[1], min_cell[2]]
                        if(min_nll < min_3[1]) min_3 <- c(min_nll, min_cell, nCnvFrags_2)
                    }
                    nlog_ks_p_alt <- round(-log10(suppressWarnings(ks.test(
                        tL, alt_cdf_3, cnvSizes[min_3[2]], cnvSizes[min_3[3]], min_3[4]/nFrags
                    )$p.value)), 3)
                    c(min(nlog_ks_p_ref, 15), min(nlog_ks_p_alt, 15),
                      cnvSizes[min_3[2]], cnvSizes[min_3[3]], min_3[4], cnvType)
                }
            }
        }
    }
    paste(res, collapse="\t")
}

# do the work and return the results
write("  calculating probablities and CNV estimates", file=stderr())
for (chrom in unique(bins$chrom)) {
    write(paste("    ", chrom, sep=""), file=stderr())
    is <- which(bins$chrom==chrom)
    bins[is,4] <- apply(bins[is,2:5], 1, parse_bin)
}
write.table(
    bins[,1:4],
    file      = "",
    quote     = FALSE,
    sep       = "\t",
    row.names = FALSE,
    col.names = FALSE
) 

#===========================================================================================



#for (libN in c('0', libraryNs)){ # '0' is for all libraries together, others start at '1'
#    dist[[libN]]$freq <- dist[[libN]]$freq * abs(dist[[libN]]$tLen)
#    dist[[libN]]$freq <- dist[[libN]]$freq / sum(dist[[libN]]$freq)
#    dist_pdf[[libN]]  <- stepfun(dist[[libN]]$tLen, c(0, dist[[libN]]$freq))    
#}

#dist_pdf[[libAll]]  <- stepfun(dist[[libAll]]$tLen, c(0, dist[[libAll]]$freq))


#lb       <- character()
#nFrags   <- 0
#ref_cum  <- numeric()
#ref_cdf  <- function(){}
#set_ref_cdf <- function(){
#    libFrac <- aggregate(lb, list(lb), function(x){ length(x) / nFrags } )
#    lfi     <- 1:nrow(libFrac)
#    ref_cum <<- cumsum(sapply(fragSizes, function(fragSize){
#        sum(sapply(lfi, function(i){
#            dist_pdf[[libFrac[i,1]]](fragSize) * libFrac[i,2]
#        }))   
#    }))
#    ref_cdf <<- stepfun(fragSizes, c(0, ref_cum))  
#}

#i_at_95    <- max(which(dist$cum <= 0.95))
#tLen_at_95 <- dist[dist$cum == dist[i_at_95,'cum'], 'tLen']
#max_del    <- max(dist$tLen) - tLen_at_95
#max_dup    <- min(dist$tLen) # negative numbers

#step_inc <- 10
#max_iter <- 50
            ## determine the type of the alternative allele and estimate MLE starting parameters            
            #tL <<- tL[order(tL)]
            #freq_at_median_null <- length(tL[tL <= median_null]) / nFrags
            #if(freq_at_median_null < median_null_freq){ # presumptive deletion
            #    cnvFrac     <- 1 - freq_at_median_null * 2
            #    cnvSize     <- tL[nFrags-1] # use second most deviant frag as initial size estimate
            #} else { # presumptive duplication
            #    cnvFrac     <- 1 - (1 - freq_at_median_null) * 2
            #    cnvSize     <- tL[2]
            #}
#min_cell  <- run_mle(max(which(dist$tLen <= cnvSize)), round(nFrags * cnvFrac))
#fill_matrix <- function(tLenI, nCnvFrags){
#    # simplify the search by using bins of tLen and nCnvFrags (i.e step functions)
#    minRow <- max(tLenI  - step_inc, 1)
#    maxRow <- min(tLenI  + step_inc, nTLens)
#    minCol <- max(nCnvFrags - step_inc, 1)
#    maxCol <- min(nCnvFrags + step_inc, nFrags)
#    outer(minRow:maxRow, minCol:maxCol, fill_matrix_cell)
#    which(nll == min(nll[minRow:maxRow,minCol:maxCol]), arr.ind=TRUE)
#}
#run_mle <- function(tLenI, nCnvFrags){
## brute force method, calculate likelihood for every possible cell
### limited method 1 = sample q3 steps on each table axis
##    outer(seq(1, nTLens, 3), seq(1, nFrags, 3), fill_matrix_cell)
##    min_cell <- which(nll == min(nll, na.rm=TRUE), arr.ind=TRUE)
##    fill_matrix(min_cell[1,1], min_cell[1,2])
### limited method 2 = make initial guess and then walk from there
#    #min_cell <- fill_matrix(tLenI, nCnvFrags)
#    ## walk the matrix, up to a point, to optimize the initial estimate
#    #i <- 1
#    #while(i <= max_iter & !(min_cell[1,1]==tLenI & min_cell[1,2]==nCnvFrags)){
#    #    tLenI     <- min_cell[1,1]
#    #    nCnvFrags <- min_cell[1,2]
#    #    min_cell  <- fill_matrix(tLenI, nCnvFrags)
#    #    i <- i + 1
#    #}
#    ## return the cell (tLen/nCnvFrag combo) corresponding to the max likelihood
#    #min_cell 
#}
        # use ks.test as a simple surrogate for true likelihood calculation
        # by building a composite CDF from the null(non-CNV) and alt (CNV) alleles
        #ks_ps[tLenI,nCnvFrags] <<- suppressWarnings(
        #    ks.test(tL, alt_cdf, nCnvFrags / nFrags, dist[tLenI, 'tLen'] - median_null)$p.value
        #)
#nll[tLenI,nCnvFrags] <<- -sum(log(dist_pdf(tL) * (1-cnvFrac) + dist_pdf(tL - cnvSize) * cnvFrac))
#library(stats4)
#upper <- c(10000, 1)  # delta frac
#minuslogl <- function(delta, frac){ # sum two weighted frequency functions
#    -sum(log(dist_pdf(tL) * (1-frac) + dist_pdf(tL + delta) * frac))
#}
#dist$pdf <- dist_pdf(dist$tLen)
#dist$cdf <- dist_cdf(dist$tLen)
#dist
#stop()
#minTLen   <- min(dist$tLen)
#maxTLen   <- max(dist$tLen)
#dist_freq <- function(tL){
#    sapply(tL, function(tL){
#        if(tL < minTLen | tL >= maxTLen){
#            0
#        } else { # linear interpolation of frequency step function for mle
#            i <- max(which(dist$tLen<=tL))
#            (dist[i+1,'freq'] - dist[i,'freq']) / (dist[i+1,'tLen'] - dist[i,'tLen']) * (tL - dist[i,'tLen']) + dist[i,'freq']  
#        } 
#    })
#}