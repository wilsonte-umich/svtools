
# get passed arguments
sample          <- Sys.getenv("sample")
maxCnvSize      <- as.numeric(Sys.getenv("maxCnvSize"))
pdfDir          <- Sys.getenv("pdfDir")
libraries       <- strsplit(Sys.getenv("libraries"), ",")[[1]]
libraryNs       <- strsplit(Sys.getenv("libraryNs"), ",")[[1]]
tLenFile        <- Sys.getenv("tLenFile")
min_ks_pairs    <- as.numeric(Sys.getenv("min_ks_pairs"))
nlog_ks_alpha   <- -log10(as.numeric(Sys.getenv("ks_alpha")))
minCnvPairs     <- as.numeric(Sys.getenv("minCnvPairs"))

# calculate derived values
minTLen <- -maxCnvSize;
maxTLen <- 2 * maxCnvSize;

# get the TLEN data for all interrogated bin positions
write("  loading bin TLEN data", file=stderr())
bins <- read.table(tLenFile, header=FALSE, sep="\t", stringsAsFactors=FALSE)
colnames(bins) <- c('chrom', 'binI', 'nFrags', 'tLens', 'libNs')

# get the expected TLEN frequency distributions
#   generated by svtools pdf for the same libraries/sample
#   '0' is for all libraries together, others start at '1'
write("  loading library TLEN distributions", file=stderr())
libAll    <- '0'
nLib      <- length(libraries)
dist      <- list()
dist_freq <- list()
load_dist <- function(lib, libN){
    distFile      <- paste(pdfDir, "/svtools.pdf.", sample, ".", lib, ".gz", sep="")
    tmp           <- read.table(distFile, header=FALSE, sep="\t", stringsAsFactors=FALSE)
    colnames(tmp) <- c('tLen', 'freq')
    dist[[libN]]      <<- tmp[tmp$tLen>=minTLen&tmp$tLen<=maxTLen,]
    dist_freq[[libN]] <<- stepfun(dist[[libN]]$tLen, c(0, dist[[libN]]$freq))    
}
for (i in 1:nLib){ load_dist(libraries[i], libraryNs[i]) }
if(nLib>1){ load_dist('all', libAll) } else { dist[[libAll]] <- dist[['1']] }

# set the bounds on allowed CNV sizes
write("  setting CNV size bounds", file=stderr())
sizeBin    <- dist[[libAll]][2,'tLen'] - dist[[libAll]][1,'tLen']
cnvSizes   <- list(
    'dup'     = seq(-maxCnvSize, 0, sizeBin),
    'del_ins' = seq(-maxCnvSize, maxCnvSize, sizeBin)
)
nCnvSizes  <- list(
    'dup'     = length(cnvSizes[['dup']]),
    'del_ins' = length(cnvSizes[['del_ins']])
)
tLens  <- seq(minTLen, maxTLen, sizeBin)
nTLens <- length(tLens)

# establish weighted frequency for non-CNV regions
#   dist has size frequency by fragment _count_
#   need to weight by abs(TLEN) to get the probablity that 
#   a given position will cross a mapped span of a given length
#   allow negative TLEN=fragSize with non-zero freq to account for occasional misalignment
write("  calculating TLEN-weighted reference distribution", file=stderr())
weightFreqs <- function(freq, tLen){
    wF <- freq * abs(tLen)
    wF / sum(wF)
}
dist[[libAll]]$freq <- weightFreqs(dist[[libAll]]$freq, dist[[libAll]]$tLen)

# establish weighted frequency for CNV regions of various sizes, per library
#   set freq to zero for negative fragment sizes (allowing for rounding errors)
write("  calculating TLEN-weighted alternative distributions", file=stderr())
dist_pdf <- list()
for (libN in libraryNs){
    dist_pdf[[libN]] <- list()    
    for (cnvSize in cnvSizes[['del_ins']]){
        fragSizes <- tLens - cnvSize
        freqs     <- ifelse(fragSizes < -sizeBin, 0, dist_freq[[libN]](fragSizes))
        wF        <- weightFreqs(freqs, tLens)
        dist_pdf[[libN]][[as.character(cnvSize)]] <- stepfun(tLens, c(0, wF))
    }    
}     
    
# set the cdf used in the first-pass test of a set of TLENs against H0
#   considers all libraries together, i.e. no attempt yet to assign pairs to libaries
all_cdf <- stepfun(dist[[libAll]]$tLen, c(0, cumsum(dist[[libAll]]$freq)))

# set the cdf used in the second-pass test of a set of TLENs against H0
#   here, frequencies are determined from the libraries that gave rise to pairs
nFrags   <- 0
lb       <- character()
libFrac  <- data.frame()
ref_pdf  <- function(){}
ref_cdf  <- function(){}
get_sized_pdf <- function(cnvSize){
    stepfun(tLens, c(0, sapply(tLens, function(tLen){
        sum(sapply(1:nrow(libFrac), function(i){
            dist_pdf[[libFrac[i,1]]][[as.character(cnvSize)]](tLen) * libFrac[i,2]
        }))   
    })))
}
set_ref_cdf <- function(){
    libFrac <<- aggregate(lb, list(lb), function(x){ length(x) / nFrags } )
    ref_pdf <<- get_sized_pdf(0)
    ref_cdf <<- stepfun(tLens, c(0, cumsum(ref_pdf(tLens))))  
}

# set the CDF used for scoring the final CNV model determined by MLE
alt_cdf <- function(){}
set_alt_cdf <- function(dupSize, dupFrac, delInsSize, delInsFrac){
    dup_pdf     <- get_sized_pdf(dupSize)
    del_ins_pdf <- get_sized_pdf(delInsSize)
    refFrac     <- 1 - dupFrac - delInsFrac
    alt_cdf <<- stepfun(tLens, c(0, cumsum(
        sapply(tLens, function(tLen){    
            ref_pdf(tLen) * refFrac + dup_pdf(tLen) * dupFrac + del_ins_pdf(tLen) * delInsFrac
        })
    ))) 
}

# 1-parameter maximum likelihood estimation for duplication
#   ignores the fact that a small number of dup frags may appear as pos TLEN
#   those may however be detected as an insertion in the same bin
tL    <- numeric()
negIs <- numeric()
posIs <- numeric()
dist_pdf_dup <- list()
fill_nll_dup <- function(cnvSizeI){
    -sum(log(dist_pdf_dup[[cnvSizeI]]))
}

#   2-parameter maximum likelihood estimation, i.e. one ref, one alt weighted by cnvFrac
dist_pdf_ref <- numeric()
dist_pdf_del_ins <- list()
fill_nll_del_ins <- Vectorize(function(cnvSizeI, cnvFrac){
    -sum(log(
        dist_pdf_ref * (1 - cnvFrac) + dist_pdf_del_ins[[cnvSizeI]] * cnvFrac
    ))
})

# perform the MLE on each bin, i.e. each discrete genome position
parse_bin <- function(bin){

    # check to see if position has sufficient data to proceed
    nFrags <<- as.numeric(bin[2])
    res <- if(nFrags < min_ks_pairs){
        c(0, 0, 0, 0, 0, 0, 'gap')
    } else {

        # use KS test to determine if bin is consistent with genome TLEN distribution
        #   do first test against merged library CDF, fastest for most bins        
        tL <<- as.numeric(strsplit(bin[3], ",")[[1]])        
        nlog_ks_p_ref <- round(-log10(suppressWarnings(ks.test(tL, all_cdf)$p.value)), 3)
        if(nlog_ks_p_ref < nlog_ks_alpha){
            c(nlog_ks_p_ref, nlog_ks_p_ref, 0, 0, 0, 0, 'ref')
        } else {

            # only if merged library CDF fails proceed on to lib-specific CDF
            lb <<- strsplit(bin[4], ",")[[1]]
            set_ref_cdf()
            nlog_ks_p_ref <- round(-log10(suppressWarnings(ks.test(tL, ref_cdf)$p.value)), 3)
            if(nlog_ks_p_ref < nlog_ks_alpha){
                c(nlog_ks_p_ref, nlog_ks_p_ref, 0, 0, 0, 0, 'ref')
            } else {    
            
                # aberrant bin, split the TLENS into neg and pos TLEN, i.e. dup and del_ins
                negIs  <<- which(tL<0)
                posIs  <<- which(tL>0)            
                lenNeg <- length(negIs)
                lenPos <- length(posIs)

                # 1D maximum likelihood estimation of duplication on negative TLENS
                dup <- if(lenNeg >= minCnvPairs){        
                    # establish alt models
                    dist_pdf_dup <<- lapply(cnvSizes[['dup']], function(cnvSize){
                        sapply(negIs, function(i){
                            dist_pdf[[lb[i]]][[as.character(cnvSize)]](tL[i])
                        })
                    })
                    # peform MLE
                    nll_1    <- sapply(1:nCnvSizes[['dup']], fill_nll_dup)
                    min_cell <- which(nll_1 == min(nll_1))[1]
                    cnvSize  <- cnvSizes[['dup']][min_cell]
                    c(cnvSize, lenNeg)                    
                } else {
                    c(0, 0)
                }            
                
                # 2D maximum likelihood estimation of deletion/insertion/reference on positive TLENS
                del_ins <- if(lenPos > 0){
                    # establish the null model, wherein fragSize = TLEN
                    dist_pdf_ref <<- sapply(posIs, function(i){
                        dist_pdf[[lb[i]]][['0']](tL[i])
                    }) 
                    # establish the alt models, wherein fragSize = TLEN - cnvSize
                    dist_pdf_del_ins <<- lapply(cnvSizes[['del_ins']], function(cnvSize){        
                        sapply(posIs, function(i){
                            dist_pdf[[lb[i]]][[as.character(cnvSize)]](tL[i])
                        })
                    })
                    # perform 2D maximum likelihood estimation of alt cnvSize and nCnvFrags
                    nll_2     <- outer(1:nCnvSizes[['del_ins']], 1:lenPos/lenPos, fill_nll_del_ins)
                    min_cell  <- which(nll_2 == min(nll_2), arr.ind=TRUE)[1,]
                    cnvSize   <- cnvSizes[['del_ins']][min_cell[1]]
                    nCnvFrags <- min_cell[2]
                    if(nCnvFrags >= minCnvPairs) { c(cnvSize, nCnvFrags) } else { c(0, 0)}
                } else {
                    c(0, 0)
                }
                    
                # revise the p-value to the new CNV model
                set_alt_cdf(dup[1], dup[2]/nFrags, del_ins[1], del_ins[2]/nFrags)
                nlog_ks_p_alt <- round(-log10(suppressWarnings(ks.test(tL, alt_cdf)$p.value)), 3)                       
                
                ## DEBUGGING
                #if(nFrags == 30){
                #    write(paste(tL[order(tL)], collapse=" "), file=stderr())
                #    df <- data.frame(tL=tLens)
                #    act_fun <- ecdf(tL)
                #    df$act  <- act_fun(tLens)
                #    df$h0   <- ref_cdf(tLens)
                #    df$ha   <- alt_cdf(tLens)
                #    write.table(
                #        df,
                #        file      = stderr(),
                #        quote     = FALSE,
                #        sep       = "\t",
                #        row.names = FALSE,
                #        col.names = FALSE
                #    )      
                #}
                
                # return the results
                #   nlog_ks_p_ref, nlog_ks_p_alt,
                #   dupSize, nDupFrag, delInsSize, nDelInsFrag
                #   binType
                c(min(nlog_ks_p_ref, 15), min(nlog_ks_p_alt, 15),
                  dup[1], dup[2], del_ins[1], del_ins[2], 'cnv')
            }
        }
    }
    paste(res, collapse="\t")
}

# do the work and return the results
write("  calculating probablities and CNV estimates", file=stderr())
for (chrom in unique(bins$chrom)) {
    write(paste("    ", chrom, sep=""), file=stderr())
    is <- which(bins$chrom==chrom)
    bins[is,4] <- apply(bins[is,2:5], 1, parse_bin)
}
write.table(
    bins[,1:4],
    file      = "",
    quote     = FALSE,
    sep       = "\t",
    row.names = FALSE,
    col.names = FALSE
) 
