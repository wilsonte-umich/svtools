
# get passed arguments
sample          <- Sys.getenv("sample")
libraries       <- strsplit(Sys.getenv("libraries"), ",")[[1]]
libraryNs       <- strsplit(Sys.getenv("libraryNs"), ",")[[1]]
pdfDir          <- Sys.getenv("pdfDir")
tLenFile        <- Sys.getenv("tLenFile")
minCnvSize      <- as.numeric(Sys.getenv("minCnvSize"))
maxCnvSize      <- as.numeric(Sys.getenv("maxCnvSize"))
cnvSizeStep     <- as.numeric(Sys.getenv("cnvSizeStep"))
min_mle_pairs   <- as.numeric(Sys.getenv("min_mle_pairs"))
min_cnv_pairs   <- as.numeric(Sys.getenv("min_cnv_pairs"))

# set boundaries on alternative (CNV) models
write("  setting CNV size bounds", file=stderr())
minTLen  <- -maxCnvSize;
maxTLen  <- 2 * maxCnvSize;
cnvTypes <- c('ref', 'del', 'ins', 'dup')
cnvSizes <- list(
    'ref' =  c(0),
    'del' =   seq( minCnvSize,  maxCnvSize, cnvSizeStep),    
    'dup' =   seq(-maxCnvSize, -minCnvSize, cnvSizeStep),
    'ins' =   seq(-maxCnvSize, -minCnvSize, cnvSizeStep),
    'all' = c(0,
              seq(-maxCnvSize, -minCnvSize, cnvSizeStep),
              seq( minCnvSize,  maxCnvSize, cnvSizeStep))
)
cnvFracs_hom <- 1
cnvFracs_het <- seq(0.35, 0.65, 0.05)
cnvFracs <- list(
    'ref' = c(0),
    'del' = c(cnvFracs_hom, cnvFracs_het),    
    'dup' = c(cnvFracs_hom, cnvFracs_het / 2), 
    'ins' = c(cnvFracs_hom, cnvFracs_het)
)

# get the expected TLEN frequency distributions
#   generated by svtools pdf for the same libraries/sample
#   '0' is for all libraries together, others start at '1'
#       this script currently only uses the 'all' model
#       maintain load of everything in case wish to add lib-specific model later
write("  loading library TLEN distributions", file=stderr())
libAll    <- '0'
nLib      <- length(libraries)
dist      <- list()
dist_freq <- list()
load_dist <- function(lib, libN){
    distFile      <- paste(pdfDir, "/svtools.pdf.", sample, ".", lib, ".gz", sep="")
    tmp           <- read.table(distFile, header=FALSE, sep="\t", stringsAsFactors=FALSE)
    colnames(tmp) <- c('tLen', 'freq')
    dist[[libN]]      <<- tmp[tmp$tLen>=minTLen&tmp$tLen<=maxTLen,]
    dist_freq[[libN]] <<- stepfun(dist[[libN]]$tLen, c(0, dist[[libN]]$freq))    
}
for (i in 1:nLib){ load_dist(libraries[i], libraryNs[i]) }
if(nLib>1){ load_dist('all', libAll) } else { dist[[libAll]] <- dist[['1']] }

# set boundaries on interrogated TLENs
write("  setting TLEN bounds", file=stderr())
tLenBin <- dist[[libAll]][2,'tLen'] - dist[[libAll]][1,'tLen']
tLens   <- seq(minTLen, maxTLen, tLenBin)

# calculate weighted frequency for crossing fragments of TLEN based on CNV size
#   dist has size frequency by fragment _count_
#   need to weight by abs(TLEN) to get the probablity that 
#   a given position will cross a mapped span of a given length
weightFreqs <- function(freq, tLen){
    wF <- freq * abs(tLen)
    wF / sum(wF)
}

# establish pure models (fracCnv==1) for all allowed cnv sizes
#   these models only consider anomalous pairs
#   not the proper pairs that map within a dup span
#   allow negative fragSize at cnvSize==0 to account for occasional misalignment
#   set CNV freq to zero at negative fragment sizes (allowing for rounding errors)
write("  constructing pure anomalous pair models for all CNV sizes", file=stderr())
pdfs <- list()
for (libN in c(libAll, libraryNs)){
    pdfs[[libN]] <- list()  
    for (cnvSize in cnvSizes[['all']]){
        fragSizes   <- tLens - cnvSize
        minFragSize <- if(cnvSize==0) { minTLen } else { -tLenBin } 
        freqs       <- ifelse(fragSizes < minFragSize, 0, dist_freq[[libN]](fragSizes))
        wF          <- weightFreqs(freqs, tLens)
        pdfs[[libN]][[as.character(cnvSize)]] <- stepfun(tLens, c(0, wF))
    }    
}
pdf_all <- pdfs[[libAll]]
pdf_ref <- pdf_all[['0']]

# establish mixed models of ref + alt for combination of cnvType, cnvSize and cnvFrac
#   values are the CDF values at all stepped tLens
#   allow negative TLEN for dup, but not for ins (not applicable to del)
write("  constructing mixed pair models for all CNV models", file=stderr())
mdl_cdfs <- list()
mdl_prms <- list()
mdl_cdfs_no_dup <- list()
mdl_prms <- list()
mdlI_wrk <- 1
mdlI_wrk_no_dup <- 1
get_mdl <- function(cnvType, cnvSize, cnvFrac){
    pdf_alt <- ifelse(             
        cnvType == 'ins' & tLens < 0,              
        0,          
        pdf_ref(tLens) * (1 - cnvFrac) + pdf_ref(tLens - cnvSize) * cnvFrac                  
    )
    cumsum(pdf_alt / sum(pdf_alt))     
}
for(cnvType in cnvTypes){
    for(cnvSize in cnvSizes[[cnvType]]){
        for(cnvFrac in cnvFracs[[cnvType]]){
            mdl_cdfs[[mdlI_wrk]] <- get_mdl(cnvType, cnvSize, cnvFrac)
            mdl_prms[[mdlI_wrk]] <- list(cnvType, cnvSize, cnvFrac)
            mdlI_wrk <- mdlI_wrk + 1
            if(cnvType != 'dup') mdlI_wrk_no_dup <- mdlI_wrk
        }  
    }
}
mdlI_ext <- 1:(mdlI_wrk-1)
mdlI_ext_no_dup <- 1:(mdlI_wrk_no_dup-1)

# define initial models used to test if a bin is non-reference
#   minCnvSize*2 places minCnvSize at the midpoint between
#   ref and alt models, i.e. will prefer alt at minCnvSize and beyond
#   1/4 of crossing pairs are anomalous at a position in a het dup
#   1/2 of crossing pairs are anomalous at a position in a het del
write("  defining initial models based on min-cnv-size", file=stderr())
mdl_init <- list()
mdl_init_no_dup <- list()
mdl_init[[1]] <- get_mdl('ref', 0,             0) 
mdl_init[[2]] <- get_mdl('del',  minCnvSize*2, 0.5)
mdl_init[[3]] <- get_mdl('dup', -minCnvSize*2, 0.25)
#mdl_init[[4]] <- get_mdl('ins', -minCnvSize*2, 0.5) # gets very jumpy to use ins in init models
mdlI_init <- 1:3
mdlI_init_no_dup <- 1:2

# perform limited AUC minimization on initial models (for speed)
#   use sum(abs(CDF diff)) as rectangular approximation of area between curves
#   where the rectangle width unit is "one TLEN step", i.e 1, and thus ignorable
cdf_act <- numeric()
fill_diff_init <- Vectorize(function(mdlI){
    sum(abs(cdf_act - mdl_init[[mdlI]]))
})

# perform extended AUC minimization on all models
fill_diff_ext <- Vectorize(function(mdlI){
    sum(abs(cdf_act - mdl_cdfs[[mdlI]]))
})

# perform the MLE on each bin, i.e. each discrete genome position
parse_bin <- function(bin){

    # check to see if position has sufficient data to proceed
    nFrags <- as.numeric(bin[2])
    res <- if(nFrags < min_mle_pairs){
        c(0, 0, 0, 0, 'gap', 'gap')
    } else {

        # first-pass AUC minimization against minimal allowable CNV type, sizes and fracs
        #   if insufficient, could expand to include a couple more fracs...
        tL           <- as.numeric(strsplit(bin[3], ",")[[1]])
        allow_dup    <- length(tL[tL<0]) >= min_cnv_pairs
        cdf_act     <<- ecdf(tL)(tLens)
        diff_init    <- fill_diff_init( if(allow_dup){ mdlI_init }else{ mdlI_init_no_dup } ) 
        mdlI         <- min(which(diff_init == min(diff_init)))
        deltaCDF_ref <- round(diff_init[1], 1)
        if(mdlI == 1){
            c(deltaCDF_ref, deltaCDF_ref, 0, 0, 'ref', 'ref')

        # extended AUC minimization against all allowable CNV sizes and fracs
        #   report results as deltaCDF_ref, deltaCDF_alt, cnvSize, cnvFrac, cnvType, cnvType2
        } else {
            diff_ext     <- fill_diff_ext( if(allow_dup){ mdlI_ext }else{ mdlI_ext_no_dup } )
            mdlI         <- which(diff_ext == min(diff_ext))[1]
            mdl_alt      <- mdl_prms[[mdlI]]
            deltaCDF_alt <- round(diff_ext[mdlI], 1)            
            cnvType2     <- if(mdl_alt[[2]]<0){ 'gain' } else { 'loss' }
            c(deltaCDF_ref, deltaCDF_alt, -mdl_alt[[2]], round(mdl_alt[[3]],2), mdl_alt[[1]], cnvType2)
        } 
    }
    paste(res, collapse="\t")
}

# get the TLEN data for all interrogated bin positions
write("  loading bin TLEN data", file=stderr())
bins <- read.table(tLenFile, header=FALSE, sep="\t", stringsAsFactors=FALSE)
colnames(bins) <- c('chrom', 'binI', 'nFrags', 'tLens', 'libNs')

# do the work and return the results
write("  calculating maximum likelihood estimates", file=stderr())
for (chrom in unique(bins$chrom)) {
    write(paste("    ", chrom, sep=""), file=stderr())
    is <- which(bins$chrom==chrom)
    bins[is,4] <- apply(bins[is,2:5], 1, parse_bin)
}
write.table(
    bins[,1:4],
    file      = "",
    quote     = FALSE,
    sep       = "\t",
    row.names = FALSE,
    col.names = FALSE
) 




#=======================================================================






# things that could be added back
#   calculate the KS p-value for ref and alt ??
#       maybe useful as metric, just don't filter on it
#   use lib-specific PDF in extended models ??
#       could improve alt models, but won't affect sensitivity for finding CNVs




#mdl_cdfs <- lapply(cnvSizesAll, function(cnvSize){
#    lapply(cnvFracs, function(cnvFrac){
#        pdf_alt <- pdf_ref(tLens) * (1 - cnvFrac) + pdf_ref(tLens - cnvSize) * cnvFrac
#        cumsum(pdf_alt / sum(pdf_alt))            
#    })
#})

#get_init_mdl <- function(cnvSize, cnvFrac){
#    sizeI <- which(cnvSizesAll == cnvSize)
#    fracI <- which(cnvFracs == cnvFrac)
#    mdl_cdfs[[sizeI]][[fracI]]
#}

            #diff_ext     <- outer(1:nCnvSizesAll, 1:nCnvFracs, fill_diff_ext)
            #min_cell     <- which(diff_ext == min(diff_ext), arr.ind=TRUE)[1,]
            #cnvSize      <- cnvSizesAll[min_cell[1]]
            #cnvFrac      <- cnvFracs[min_cell[2]]
            #deltaCDF_alt <- round(diff_ext[min_cell[1], min_cell[2]], 1)
            

    #stepfun(tLens, c(0, pdf_all[[as.character(cnvSize)]](tLens)))

    #cnvFrac <- init_cnvFracs[mdlI]
    #refFrac <- 1 - cnvFrac  
    #-sum(log(
    #    mdl_ps_ref * refFrac + mdl_ps_alt[[mdlI]] * cnvFrac
    #))

    #cnvFrac <- cnvFracs[sizeI]
    #refFrac <- 1 - cnvFrac
    #-sum(log(
    #    mdl_ps_ref * refFrac + mdl_ps_alt[[sizeI]] * cnvFrac
    #))
    
    #mdl_ps_alt <<- lapply(1:nCnvSizesAll, function(sizeI){ mdl_pdfs[[sizeI]](tL) })
    
    
#mdl_ps_alt <- list()
#mdl_ps_ref <- numeric()
    
        #mdl_ps_alt <<- lapply(init_sizeIs, function(sizeI){ mdl_pdfs[[sizeI]](tL) })        
        #mdl_ps_ref <<- mdl_ps_alt[[1]]
        
            #deltaMle <- round(max(nll_init[1]-nll_init[2], nll_init[1]-nll_init[3]), 1)
            #deltaMle <- if(is.na(deltaMle)){ -10 } else { deltaMle }

            #deltaMle    <- round(nll_init[1]-nll_ext[sizeI], 1)
            #deltaMle    <- if(is.na(deltaMle)){ 10 } else { deltaMle }